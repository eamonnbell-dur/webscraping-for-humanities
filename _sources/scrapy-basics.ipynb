{"cells":[{"cell_type":"markdown","metadata":{},"source":["# `scrapy` basics\n","\n","See [the scrapy tutorial online here](https://docs.scrapy.org/en/latest/intro/tutorial.html)"]},{"cell_type":"markdown","metadata":{},"source":["In this lesson we are going to explore Scrapy (`scrapy`), a Python library for specifying and running web scraping tasks. Codes that perform the systematic retrieval of remote resources are often called spiders or crawlers. Early examples of crawlers were those that were used to populate the search indexes of search engines like Altavista or Google."]},{"cell_type":"markdown","metadata":{},"source":["What `scrapy` provides that `bs4` does not is a principled way to describe a scraping task from beginning to end. `bs4` focuses on manipulating a HTML (or HTML-like) document at hand; `scrapy` combines the retrieval step (which we did manually last time with a library like `requests`) and the extract step (which we did with `bs4`) into one artifact. Our objective is to replicate - with `scrapy` the scraping process sketched out in [`bs4` Further Topics](https://eamonnbell-dur.github.io/webscraping-for-humanities/bs4-further-topics.html). This introduction very closely follows the [Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)."]},{"cell_type":"markdown","metadata":{},"source":["The first thing to do is to initalise a `scrapy` project. You can do this at the commandline or in the cell below (`!` in a notebook cell passes the command to the shell - `bash` or similar)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!scrapy startproject basics"]},{"cell_type":"markdown","metadata":{},"source":["As the message suggests, this has created a new directory (`basics`) in the current working directory of the notebook. In order to find out where this folder is, click `File > Open...` just underneath the Jupyter logo."]},{"cell_type":"markdown","metadata":{},"source":["Now, enter the `basics/basics/spiders` folder and create a new file using the `New` button near the top right of the screen. Pick `Text File`. By clicking on the title, rename the file to `quotes_spider.py` and copy and paste the following script into the file:"]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 1\n","\n","---\n","\n","```python\n","\n","from pathlib import Path\n","\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        page = response.url.split(\"/\")[-2]\n","        filename = f'quotes-{page}.html'\n","        Path(filename).write_bytes(response.body)\n","        self.log(f'Saved file {filename}')\n","```"]},{"cell_type":"markdown","metadata":{},"source":["In this script, notwithstanding some of the Python details, we can see there are two key tasks described: `start_requests`, which prepares a `scrapy.Request` for each URI in the list `urls`. The `callback=` argument suggests that the function `parse()` is called once for each of these requests - we might guess this happens after the HTTP request to the URI has been fired, and response has been recieved. Then - for each HTTP request (response) - four things happen, in order:\n","\n","1. A bit of string processing puts the current page into a variable `page`.\n","2. We construct a filename using this variable.\n","3. We write the body of the HTTP response to a file on disk with this name.\n","4. We announce this fact to the world, via the `self.log()` function.\n","\n","At this point, we could have done this with the Python standard library, or `requests`, or some combination of both of these. The smart thing about Scrapy is the way it passes information from the response to a request into functions for later processing (or, as we will later see, for firing off further requests).\n","\n","Notice that we've given the spider a name: `\"quotes\"`. Because of the structure of the file and the directory tree that we created when we created the Scrapy project, we have a convenient way of running this spider, and we get nice logging for free. This is unlike when we work with `requests` alone. \n","\n","The command is `scrapy crawl [[spider_name]]`. (Note we have to `cd` into the project directory before we kick anything off)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl quotes"]},{"cell_type":"markdown","metadata":{},"source":["If we parse the logs, we can see the message `Saved file quotes-1.html`. Similarly, we can double check that these files have been downloaded. (`head -n 20` shows the first ten lines of a file)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head -n 20 quotes-1.html"]},{"cell_type":"markdown","metadata":{},"source":["In order to better understand the power of Scrapy, we are going to modify  `quotes_spider.py` just a little bit. This isn't something we'd do normally, but it serves to illustrate a point. Instead of writing the response body to a file, let's bundle a snippet of it (the first 10 characters) up with a little bit of metadata - in this case, the URI for the resource, the filename that the resource would have had. Scrapy uses the `yield` keyword to achieve this. "]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 2\n","\n","---\n","\n","\n","```python\n","\n","from pathlib import Path\n","\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        page = response.url.split(\"/\")[-2]\n","        filename = f'quotes-{page}.html'\n","        \n","        yield {\n","            'filename': filename,\n","            'uri': response.url,\n","            'body_snippet': response.body[:10]\n","        }\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl quotes -O quotes.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head quotes.csv"]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 3\n","\n","---\n","\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        yield {\n","            'uri': response.url,\n","            'title': soup.title.text,\n","        }\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl quotes -O quotes.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head quotes.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["At this point, you probably have enough to [go back to the final result with `bs4` from the last day's workshop](https://eamonnbell-dur.github.io/webscraping-for-humanities/bs4-further-topics.html#) and use what you know to create a new spider, called `discogs_spider.py`, which, given a URI of a Discogs.com list, will **yield** the album titles and the links to the cover images for each album in the list.\n","\n","### Source for `discogs_spider.py` \n","\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{"tags":[]},"source":["```{toggle}\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class DiscogsSpider(scrapy.Spider):\n","    name = \"discogs\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://www.discogs.com/lists/277616',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        ol_albums = soup.find('ol', id='listitems')\n","        li_albums = ol_albums.find_all('li')\n","\n","        \n","        for li in li_albums:\n","            album_title = li.find('a').get_text()\n","            cover_image_link = li.find('img')['src'] \n","            yield {\n","                'uri': response.url,\n","                'album_title': album_title,\n","                'cover_image_link': cover_image_link\n","            }\n","```\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl discogs -O discogs.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head discogs.csv"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"790dd6c57fc929fd42e5ab267bfea63b216295e4d85fc21cf4d3027566e8b5a9"}}},"nbformat":4,"nbformat_minor":2}
