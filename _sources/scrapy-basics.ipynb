{"cells":[{"cell_type":"markdown","metadata":{},"source":["# `scrapy` basics\n","\n","See [the scrapy tutorial online here](https://docs.scrapy.org/en/latest/intro/tutorial.html)"]},{"cell_type":"markdown","metadata":{},"source":["In this lesson we are going to explore Scrapy (`scrapy`), a Python library for specifying and running web scraping tasks. Codes that perform the systematic retrieval of remote resources are often called spiders or crawlers. Early examples of crawlers were those that were used to populate the search indexes of search engines like Altavista or Google."]},{"cell_type":"markdown","metadata":{},"source":["What `scrapy` provides that `bs4` does not is a principled way to describe a scraping task from beginning to end. `bs4` focuses on manipulating a HTML (or HTML-like) document at hand; `scrapy` combines the retrieval step (which we did manually last time with a library like `requests`) and the extract step (which we did with `bs4`) into one artifact. Our objective is to replicate - with `scrapy` the scraping process sketched out in [`bs4` Further Topics](https://eamonnbell-dur.github.io/webscraping-for-humanities/bs4-further-topics.html). This introduction very closely follows the [Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)."]},{"cell_type":"markdown","metadata":{},"source":["The first thing to do is to initalise a `scrapy` project. You can do this at the commandline or in the cell below (`!` in a notebook cell passes the command to the shell - `bash` or similar)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!scrapy startproject basics"]},{"cell_type":"markdown","metadata":{},"source":["As the message suggests, this has created a new directory (`basics`) in the current working directory of the notebook. In order to find out where this folder is, click `File > Open...` just underneath the Jupyter logo."]},{"cell_type":"markdown","metadata":{},"source":["Now, enter the `basics/basics/spiders` folder and create a new file using the `New` button near the top right of the screen. Pick `Text File`. By clicking on the title, rename the file to `quotes_spider.py` and copy and paste the following script into the file:"]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 1\n","\n","---\n","\n","```python\n","\n","from pathlib import Path\n","\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        page = response.url.split(\"/\")[-2]\n","        filename = f'quotes-{page}.html'\n","        Path(filename).write_bytes(response.body)\n","        self.log(f'Saved file {filename}')\n","```"]},{"cell_type":"markdown","metadata":{},"source":["In this script, notwithstanding some of the Python details, we can see there are two key tasks described: `start_requests`, which prepares a `scrapy.Request` for each URI in the list `urls`. The `callback=` argument suggests that the function `parse()` is called once for each of these requests - we might guess this happens after the HTTP request to the URI has been fired, and response has been recieved. Then - for each HTTP request (response) - four things happen, in order:\n","\n","1. A bit of string processing puts the current page into a variable `page`.\n","2. We construct a filename using this variable.\n","3. We write the body of the HTTP response to a file on disk with this name.\n","4. We announce this fact to the world, via the `self.log()` function.\n","\n","At this point, we could have done this with the Python standard library, or `requests`, or some combination of both of these. The smart thing about Scrapy is the way it passes information from the response to a request into functions for later processing (or, as we will later see, for firing off further requests).\n","\n","Notice that we've given the spider a name: `\"quotes\"`. Because of the structure of the file and the directory tree that we created when we created the Scrapy project, we have a convenient way of running this spider, and we get nice logging for free. This is unlike when we work with `requests` alone. \n","\n","The command is `scrapy crawl [[spider_name]]`. (Note we have to `cd` into the project directory before we kick anything off)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl quotes"]},{"cell_type":"markdown","metadata":{},"source":["If we parse the logs, we can see the message `Saved file quotes-1.html`. Similarly, we can double check that these files have been downloaded. (`head -n 20` shows the first ten lines of a file)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head -n 20 quotes-1.html"]},{"cell_type":"markdown","metadata":{},"source":["In order to better understand the power of Scrapy, we are going to modify  `quotes_spider.py` just a little bit. This isn't something we'd do normally, but it serves to illustrate a point. Instead of writing the response body to a file, let's bundle a snippet of it (the first 10 characters) up with a little bit of metadata - in this case, the URI for the resource, the filename that the resource would have had. Scrapy uses the `yield` keyword to achieve this. "]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 2\n","\n","---\n","\n","\n","```python\n","\n","from pathlib import Path\n","\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        page = response.url.split(\"/\")[-2]\n","        filename = f'quotes-{page}.html'\n","        \n","        yield {\n","            'filename': filename,\n","            'uri': response.url,\n","            'body_snippet': response.body[:10]\n","        }\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl quotes -O quotes.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head quotes.csv"]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 3\n","\n","---\n","\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        yield {\n","            'uri': response.url,\n","            'title': soup.title.text,\n","        }\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl quotes -O quotes.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head quotes.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["At this point, you probably have enough to [go back to the final result with `bs4` from the last day's workshop](https://eamonnbell-dur.github.io/webscraping-for-humanities/bs4-further-topics.html#) and use what you know to create a new spider, called `discogs_spider.py`, which, given a URI of a Discogs.com list, will **yield** the album titles and the links to the cover images for each album in the list.\n","\n","Think about how the script in `quotes_spider.py` could be adapted to work for Discogs.com before revealing the answer."]},{"attachments":{},"cell_type":"markdown","metadata":{"tags":[]},"source":["### Source for `discogs_spider.py` - version 1\n","\n","---\n","\n","```{toggle}\n","\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class DiscogsSpider(scrapy.Spider):\n","    name = \"discogs\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://www.discogs.com/lists/277616',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        ol_albums = soup.find('ol', id='listitems')\n","        li_albums = ol_albums.find_all('li')\n","\n","        for li in li_albums:\n","            album_title = li.find('a').get_text()\n","            cover_image_link = li.find('img')['src'] \n","            yield {\n","                'uri': response.url,\n","                'album_title': album_title,\n","                'cover_image_link': cover_image_link\n","            }\n","```\n","\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's run the crawler..."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; scrapy crawl discogs -O discogs.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["...and look at the data it produces:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!cd basics; head discogs.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We notice that the data returned relates only to the albums listed on the first page. We need to update the spider in such a way that it:\n","\n","1. Identifies the link to the next page of results\n","2. Fires off a HTTP request to the URI that this link points to \n","3. Parses the body of this request to extract the desired information\n","4. Go to step 1, stopping there if the link to the next page cannot be found (this means that we are on the last page of results)\n","\n","Have a go at adapting the scraper for this task.\n","\n","```{admonition} Hint\n",":class: tip\n","We know that the `yield` keyword is used inside `parse()` to expose the data that is transformed by the HTML parser so that it can be output to the user each time the spider is run. We also know that inside `start_requests()` the `yield` keyword can be used to trigger a request. There is no constraint on the number of `yield` statements in a function body; Python will get to each of them in turn. \n","\n","Python generators are a out of scope, [but you can read a bit about how they work here](https://wiki.python.org/moin/Generators) if you are interested. If you already know about them, [read the original proposal (PEP 255) that introduced them to the language](https://peps.python.org/pep-0255/).\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Source for `discogs_spider.py` - version 2\n","\n","---\n","\n","```{toggle}\n","\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class DiscogsSpider(scrapy.Spider):\n","    name = \"discogs\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://www.discogs.com/lists/277616',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        ol_albums = soup.find('ol', id='listitems')\n","        li_albums = ol_albums.find_all('li')\n","\n","        for li in li_albums:\n","            album_title = li.find('a').get_text()\n","            cover_image_link = li.find('img')['src'] \n","            yield {\n","                'uri': response.url,\n","                'album_title': album_title,\n","                'cover_image_link': cover_image_link\n","            }\n","\n","        nav = soup.find('nav', attrs={'aria-label':'Pagination'})\n","        link = nav.find('a', class_='pagination_next')\n","        link_destination = link.attrs['href']\n","\n","        if link_destination is not None:\n","            absolute_next_link = 'https://www.discogs.com' + link_destination\n","            yield scrapy.Request(absolute_next_link, callback=self.parse)\n","```\n","\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["(Don't forget to update your `basics/basics/spiders/discogs_scraper.py` file before proceeding!). Let's run the new scraper!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd basics; scrapy crawl discogs -O discogs.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If we count the number of lines in the file, we'll see that over 150 results were retrieved. And, crucially, the scraper stopped correctly when it found that there was no next link on the page. Making sure that your spider has a well-defined stopping condition is key."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cd basics; wc -l discogs.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["So we have arrived pretty much where we were at the end of the last tutorial, working instead with the Scrapy framework to describe a crawler. There are many advantages to this, but perhaps one of the most significant is the way that `scrapy` splits the responsibility for the scraping-related tasks into independent subsystems. Here is a picture from the Scrapy authors depicting its large-scale design:\n","\n","![https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png](https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["One of the things that this allows for is the declarative specification of settings for the scraper, whether at the project level or at the level of the individual scraper. **Correctly setting these values is crucial for responsible scraping**. You can find the global settings inside the file `basics/basics/settings.py`. I exceprt some of the interesting settings below as they relate to:\n","\n","- Setting the `USER_AGENT` string to identify yourself and your crawler\n","- Setting the crawler to respect `robots.txt`\n","- Setting up limits on the number of concurrent requests to remote resources (globally and on a per domain basis)\n","- Introduce a delay between requests (a basic form of ratelimiting/throttling) (see also: [the Scrapy documentation about Autothrottling](https://docs.scrapy.org/en/latest/topics/autothrottle.html?highlight=autothrottle))\n","\n","I also include a setting about the encoding of the exported data, which may be relevant to you depending on the sites you are interested in studying.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["```python\n","# Crawl responsibly by identifying yourself (and your website) on the user-agent\n","# Uncomment this line and fill in your own value here\n","#USER_AGENT = \"basics (+http://www.yourdomain.com)\"\n","\n","# Obey robots.txt rules\n","ROBOTSTXT_OBEY = True\n","\n","# Configure maximum concurrent requests performed by Scrapy (default: 16)\n","CONCURRENT_REQUESTS = 32\n","# Configure maximum concurrent requests per domain performed by Scrapy \n","CONCURRENT_REQUESTS_PER_DOMAIN = 16\n","\n","# Configure a delay for requests for the same website (default: 0)\n","DOWNLOAD_DELAY = 3\n","\n","# Set the encoding format for your item exports (this is a good choice but in case you\n","# are working with CJK sites this could need tweaking).\n","FEED_EXPORT_ENCODING = \"utf-8\"\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In order to extend the scope of this scraper, an obvious focus would be the list of URIs provided in the spider to get the job running. Here we just use one URI, which acts as a \"seed\" for the spider, from which all the other requests \"grow\". But we may have several thousands of URIs to work with. As ever, there are many ways to move forward: these could either requested through repeated calls to the `scrapy.Request()` constructor, they could be provided as command-line arguments to the command that starts the crawl, or they could be retrieved from a global queue of URIs to process. These topics are out of scope for today, but I hope that you now have enough of an interest in Scrapy to continue exploring!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"790dd6c57fc929fd42e5ab267bfea63b216295e4d85fc21cf4d3027566e8b5a9"}}},"nbformat":4,"nbformat_minor":2}
