{"cells":[{"cell_type":"markdown","metadata":{},"source":["# `scrapy` basics\n","\n","See [the scrapy tutorial online here](https://docs.scrapy.org/en/latest/intro/tutorial.html)"]},{"cell_type":"markdown","metadata":{},"source":["In this lesson we are going to explore Scrapy (`scrapy`), a Python library for specifying and running web scraping tasks. Codes that perform the systematic retrieval of remote resources are often called spiders or crawlers. Early examples of crawlers were those that were used to populate the search indexes of search engines like Altavista or Google."]},{"cell_type":"markdown","metadata":{},"source":["What `scrapy` provides that `bs4` does not is a principled way to describe a scraping task from beginning to end. `bs4` focuses on manipulating a HTML (or HTML-like) document at hand; `scrapy` combines the retrieval step (which we did manually last time with a library like `requests`) and the extract step (which we did with `bs4`) into one artifact. Our objective is to replicate - with `scrapy` the scraping process sketched out in [`bs4` Further Topics](https://eamonnbell-dur.github.io/webscraping-for-humanities/bs4-further-topics.html). This introduction very closely follows the [Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)."]},{"cell_type":"markdown","metadata":{},"source":["The first thing to do is to initalise a `scrapy` project. You can do this at the commandline or in the cell below (`!` in a notebook cell passes the command to the shell - `bash` or similar)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["New Scrapy project 'basics', using template directory '/srv/conda/envs/notebook/lib/python3.7/site-packages/scrapy/templates/project', created in:\r\n","    /home/jovyan/basics\r\n","\r\n","You can start your first spider with:\r\n","    cd basics\r\n","    scrapy genspider example example.com\r\n"]}],"source":["!scrapy startproject basics"]},{"cell_type":"markdown","metadata":{},"source":["As the message suggests, this has created a new directory (`basics`) in the current working directory of the notebook. In order to find out where this folder is, click `File > Open...` just underneath the Jupyter logo."]},{"cell_type":"markdown","metadata":{},"source":["Now, enter the `basics/basics/spiders` folder and create a new file using the `New` button near the top right of the screen. Pick `Text File`. By clicking on the title, rename the file to `quotes_spider.py` and copy and paste the following script into the file:"]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 1\n","\n","---\n","\n","```python\n","\n","from pathlib import Path\n","\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        page = response.url.split(\"/\")[-2]\n","        filename = f'quotes-{page}.html'\n","        Path(filename).write_bytes(response.body)\n","        self.log(f'Saved file {filename}')\n","```"]},{"cell_type":"markdown","metadata":{},"source":["In this script, notwithstanding some of the Python details, we can see there are two key tasks described: `start_requests`, which prepares a `scrapy.Request` for each URI in the list `urls`. The `callback=` argument suggests that the function `parse()` is called once for each of these requests - we might guess this happens after the HTTP request to the URI has been fired, and response has been recieved. Then - for each HTTP request (response) - four things happen, in order:\n","\n","1. A bit of string processing puts the current page into a variable `page`.\n","2. We construct a filename using this variable.\n","3. We write the body of the HTTP response to a file on disk with this name.\n","4. We announce this fact to the world, via the `self.log()` function.\n","\n","At this point, we could have done this with the Python standard library, or `requests`, or some combination of both of these. The smart thing about Scrapy is the way it passes information from the response to a request into functions for later processing (or, as we will later see, for firing off further requests).\n","\n","Notice that we've given the spider a name: `\"quotes\"`. Because of the structure of the file and the directory tree that we created when we created the Scrapy project, we have a convenient way of running this spider, and we get nice logging for free. This is unlike when we work with `requests` alone. \n","\n","The command is `scrapy crawl [[spider_name]]`. (Note we have to `cd` into the project directory before we kick anything off)."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-02-06 22:47:30 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: basics)\n","2023-02-06 22:47:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.2, Platform Linux-5.10.133+-x86_64-with-debian-buster-sid\n","2023-02-06 22:47:30 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'basics',\n"," 'FEED_EXPORT_ENCODING': 'utf-8',\n"," 'NEWSPIDER_MODULE': 'basics.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['basics.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2023-02-06 22:47:30 [asyncio] DEBUG: Using selector: EpollSelector\n","2023-02-06 22:47:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2023-02-06 22:47:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2023-02-06 22:47:30 [scrapy.extensions.telnet] INFO: Telnet Password: b35e8e6268cdefdc\n","2023-02-06 22:47:30 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2023-02-06 22:47:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2023-02-06 22:47:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2023-02-06 22:47:30 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2023-02-06 22:47:30 [scrapy.core.engine] INFO: Spider opened\n","2023-02-06 22:47:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2023-02-06 22:47:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2023-02-06 22:47:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n","2023-02-06 22:47:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n","2023-02-06 22:47:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n","2023-02-06 22:47:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n","{'filename': 'quotes-1.html', 'uri': 'https://quotes.toscrape.com/page/1/', 'body_snippet': b'<!DOCTYPE html>\\n<htm'}\n","2023-02-06 22:47:31 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n","{'filename': 'quotes-2.html', 'uri': 'https://quotes.toscrape.com/page/2/', 'body_snippet': b'<!DOCTYPE html>\\n<htm'}\n","2023-02-06 22:47:31 [scrapy.core.engine] INFO: Closing spider (finished)\n","2023-02-06 22:47:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 693,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 5555,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/404': 1,\n"," 'elapsed_time_seconds': 0.40816,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2023, 2, 6, 22, 47, 31, 161763),\n"," 'httpcompression/response_bytes': 25019,\n"," 'httpcompression/response_count': 3,\n"," 'item_scraped_count': 2,\n"," 'log_count/DEBUG': 8,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 65552384,\n"," 'memusage/startup': 65552384,\n"," 'response_received_count': 3,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/404': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2023, 2, 6, 22, 47, 30, 753603)}\n","2023-02-06 22:47:31 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["!cd basics; scrapy crawl quotes"]},{"cell_type":"markdown","metadata":{},"source":["If we parse the logs, we can see the message `Saved file quotes-1.html`. Similarly, we can double check that these files have been downloaded. (`head -n 20` shows the first ten lines of a file)."]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<!DOCTYPE html>\r\n","<html lang=\"en\">\r\n","<head>\r\n","\t<meta charset=\"UTF-8\">\r\n","\t<title>Quotes to Scrape</title>\r\n","    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\r\n","    <link rel=\"stylesheet\" href=\"/static/main.css\">\r\n","</head>\r\n","<body>\r\n","    <div class=\"container\">\r\n","        <div class=\"row header-box\">\r\n","            <div class=\"col-md-8\">\r\n","                <h1>\r\n","                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\r\n","                </h1>\r\n","            </div>\r\n","            <div class=\"col-md-4\">\r\n","                <p>\r\n","                \r\n","                    <a href=\"/login\">Login</a>\r\n"]}],"source":["!cd basics; head -n 20 quotes-1.html"]},{"cell_type":"markdown","metadata":{},"source":["In order to better understand the power of Scrapy, we are going to modify  `quotes_spider.py` just a little bit. This isn't something we'd do normally, but it serves to illustrate a point. Instead of writing the response body to a file, let's bundle a snippet of it (the first 10 characters) up with a little bit of metadata - in this case, the URI for the resource, the filename that the resource would have had. Scrapy uses the `yield` keyword to achieve this. "]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 2\n","\n","---\n","\n","\n","```python\n","\n","from pathlib import Path\n","\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        page = response.url.split(\"/\")[-2]\n","        filename = f'quotes-{page}.html'\n","        \n","        yield {\n","            'filename': filename,\n","            'uri': response.url,\n","            'body_snippet': response.body[:10]\n","        }\n","```"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-02-06 22:49:24 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: basics)\n","2023-02-06 22:49:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.2, Platform Linux-5.10.133+-x86_64-with-debian-buster-sid\n","2023-02-06 22:49:24 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'basics',\n"," 'FEED_EXPORT_ENCODING': 'utf-8',\n"," 'NEWSPIDER_MODULE': 'basics.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['basics.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2023-02-06 22:49:24 [asyncio] DEBUG: Using selector: EpollSelector\n","2023-02-06 22:49:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2023-02-06 22:49:24 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2023-02-06 22:49:24 [scrapy.extensions.telnet] INFO: Telnet Password: 306d6273762e5e75\n","2023-02-06 22:49:24 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2023-02-06 22:49:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2023-02-06 22:49:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2023-02-06 22:49:24 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2023-02-06 22:49:24 [scrapy.core.engine] INFO: Spider opened\n","2023-02-06 22:49:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2023-02-06 22:49:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2023-02-06 22:49:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n","2023-02-06 22:49:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n","2023-02-06 22:49:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n","2023-02-06 22:49:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n","{'filename': 'quotes-1.html', 'uri': 'https://quotes.toscrape.com/page/1/', 'body_snippet': b'<!DOCTYPE html>\\n<htm'}\n","2023-02-06 22:49:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n","{'filename': 'quotes-2.html', 'uri': 'https://quotes.toscrape.com/page/2/', 'body_snippet': b'<!DOCTYPE html>\\n<htm'}\n","2023-02-06 22:49:25 [scrapy.core.engine] INFO: Closing spider (finished)\n","2023-02-06 22:49:25 [scrapy.extensions.feedexport] INFO: Stored csv feed (2 items) in: quotes.csv\n","2023-02-06 22:49:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 693,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 5556,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/404': 1,\n"," 'elapsed_time_seconds': 0.355137,\n"," 'feedexport/success_count/FileFeedStorage': 1,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2023, 2, 6, 22, 49, 25, 298707),\n"," 'httpcompression/response_bytes': 25019,\n"," 'httpcompression/response_count': 3,\n"," 'item_scraped_count': 2,\n"," 'log_count/DEBUG': 8,\n"," 'log_count/INFO': 11,\n"," 'memusage/max': 65564672,\n"," 'memusage/startup': 65564672,\n"," 'response_received_count': 3,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/404': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2023, 2, 6, 22, 49, 24, 943570)}\n","2023-02-06 22:49:25 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["!cd basics; scrapy crawl quotes -O quotes.csv"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["filename,uri,body_snippet\r\r\n","quotes-1.html,https://quotes.toscrape.com/page/1/,\"<!DOCTYPE html>\r\n","<htm\"\r\r\n","quotes-2.html,https://quotes.toscrape.com/page/2/,\"<!DOCTYPE html>\r\n","<htm\"\r\r\n"]}],"source":["!cd basics; head quotes.csv"]},{"cell_type":"markdown","metadata":{},"source":["### Source for `quotes_spider.py` - version 3\n","\n","---\n","\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class QuotesSpider(scrapy.Spider):\n","    name = \"quotes\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://quotes.toscrape.com/page/1/',\n","            'https://quotes.toscrape.com/page/2/',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        yield {\n","            'uri': response.url,\n","            'title': soup.title.text,\n","        }\n","```"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-02-06 22:53:23 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: basics)\n","2023-02-06 22:53:23 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.2, Platform Linux-5.10.133+-x86_64-with-debian-buster-sid\n","2023-02-06 22:53:23 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'basics',\n"," 'FEED_EXPORT_ENCODING': 'utf-8',\n"," 'NEWSPIDER_MODULE': 'basics.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['basics.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2023-02-06 22:53:23 [asyncio] DEBUG: Using selector: EpollSelector\n","2023-02-06 22:53:23 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2023-02-06 22:53:23 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2023-02-06 22:53:23 [scrapy.extensions.telnet] INFO: Telnet Password: edbd6ead82e60af5\n","2023-02-06 22:53:23 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2023-02-06 22:53:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2023-02-06 22:53:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2023-02-06 22:53:23 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2023-02-06 22:53:23 [scrapy.core.engine] INFO: Spider opened\n","2023-02-06 22:53:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2023-02-06 22:53:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2023-02-06 22:53:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n","2023-02-06 22:53:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n","2023-02-06 22:53:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n","2023-02-06 22:53:23 [py.warnings] WARNING: /home/jovyan/basics/basics/spiders/quotes_spider.py:19: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 19 of the file /home/jovyan/basics/basics/spiders/quotes_spider.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  soup = bs4.BeautifulSoup(response.text)\n","\n","2023-02-06 22:53:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n","{'uri': 'https://quotes.toscrape.com/page/1/', 'title': <title>Quotes to Scrape</title>}\n","2023-02-06 22:53:23 [py.warnings] WARNING: /home/jovyan/basics/basics/spiders/quotes_spider.py:19: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 19 of the file /home/jovyan/basics/basics/spiders/quotes_spider.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  soup = bs4.BeautifulSoup(response.text)\n","\n","2023-02-06 22:53:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n","{'uri': 'https://quotes.toscrape.com/page/2/', 'title': <title>Quotes to Scrape</title>}\n","2023-02-06 22:53:23 [scrapy.core.engine] INFO: Closing spider (finished)\n","2023-02-06 22:53:23 [scrapy.extensions.feedexport] INFO: Stored csv feed (2 items) in: quotes.csv\n","2023-02-06 22:53:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 693,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 5557,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/404': 1,\n"," 'elapsed_time_seconds': 0.450085,\n"," 'feedexport/success_count/FileFeedStorage': 1,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2023, 2, 6, 22, 53, 23, 752230),\n"," 'httpcompression/response_bytes': 25019,\n"," 'httpcompression/response_count': 3,\n"," 'item_scraped_count': 2,\n"," 'log_count/DEBUG': 8,\n"," 'log_count/INFO': 11,\n"," 'log_count/WARNING': 2,\n"," 'memusage/max': 67162112,\n"," 'memusage/startup': 67162112,\n"," 'response_received_count': 3,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/404': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2023, 2, 6, 22, 53, 23, 302145)}\n","2023-02-06 22:53:23 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["!cd basics; scrapy crawl quotes -O quotes.csv"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["uri,title\r\r\n","https://quotes.toscrape.com/page/1/,<title>Quotes to Scrape</title>\r\r\n","https://quotes.toscrape.com/page/2/,<title>Quotes to Scrape</title>\r\r\n"]}],"source":["!cd basics; head quotes.csv"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["At this point, you probably have enough to [go back to the final result with `bs4` from the last day's workshop](https://eamonnbell-dur.github.io/webscraping-for-humanities/bs4-further-topics.html#) and use what you know to create a new spider, called `discogs_spider.py`, which, given a URI of a Discogs.com list, will **yield** the album titles and the links to the cover images for each album in the list."]},{"cell_type":"markdown","metadata":{"tags":["hide-cell"]},"source":["### Source for `discogs_spider.py` \n","\n","---\n","\n","```python\n","from pathlib import Path\n","\n","import bs4\n","import scrapy\n","\n","\n","class DiscogsSpider(scrapy.Spider):\n","    name = \"discogs\"\n","\n","    def start_requests(self):\n","        urls = [\n","            'https://www.discogs.com/lists/277616',\n","        ]\n","        for url in urls:\n","            yield scrapy.Request(url=url, callback=self.parse)\n","\n","    def parse(self, response):\n","        soup = bs4.BeautifulSoup(response.text)\n","        \n","        ol_albums = soup.find('ol', id='listitems')\n","        li_albums = ol_albums.find_all('li')\n","\n","        \n","        for li in li_albums:\n","            album_title = li.find('a').get_text()\n","            cover_image_link = li.find('img')['src'] \n","            yield {\n","                'uri': response.url,\n","                'album_title': album_title,\n","                'cover_image_link': cover_image_link\n","            }\n","```"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-02-06 23:02:34 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: basics)\n","2023-02-06 23:02:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) - [GCC 9.4.0], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 38.0.2, Platform Linux-5.10.133+-x86_64-with-debian-buster-sid\n","2023-02-06 23:02:34 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'basics',\n"," 'FEED_EXPORT_ENCODING': 'utf-8',\n"," 'NEWSPIDER_MODULE': 'basics.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['basics.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2023-02-06 23:02:34 [asyncio] DEBUG: Using selector: EpollSelector\n","2023-02-06 23:02:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2023-02-06 23:02:34 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2023-02-06 23:02:34 [scrapy.extensions.telnet] INFO: Telnet Password: 1552ba76b2534ec8\n","2023-02-06 23:02:34 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2023-02-06 23:02:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2023-02-06 23:02:34 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2023-02-06 23:02:34 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2023-02-06 23:02:34 [scrapy.core.engine] INFO: Spider opened\n","2023-02-06 23:02:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2023-02-06 23:02:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2023-02-06 23:02:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.discogs.com/robots.txt> (referer: None)\n","2023-02-06 23:02:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.discogs.com/lists/277616> (referer: None)\n","2023-02-06 23:02:35 [py.warnings] WARNING: /home/jovyan/basics/basics/spiders/discogs_spider.py:18: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 18 of the file /home/jovyan/basics/basics/spiders/discogs_spider.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  soup = bs4.BeautifulSoup(response.text)\n","\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Maroon 5 - Songs About Jane', 'cover_image_link': 'https://i.discogs.com/9S5gwiP7Jr5Emfdrl8ERBHxUAc4p-mPHU8GMbGbGees/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTQzNTEz/OC0xMTEzMDM2MzE1/LmpwZw.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Hawkwind - Space Ritual', 'cover_image_link': 'https://i.discogs.com/wMzYu3lEPnNPB8Ord6SyhoA6eZSvgu8ZQMSxRxO34Dw/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTY0MDQw/Ni0xNjA4MTM0NTcx/LTE0NzkuanBlZw.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Cave Gaze Wagon - Wonderful Wagon World', 'cover_image_link': 'https://i.discogs.com/wyMbzqKbQSMjcdkWowSrSRX7Pj_m5phhSxw5ZFe3oE4/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgxNTk3/OTctMTQ1NjI1MjI2/Ni00MDg1LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'The Pax Cecilia - Nouveau (A Theatre Of The Air)', 'cover_image_link': 'https://i.discogs.com/tfzq-AQIFomVcMDxegx4a8pVCgjn9_2CdDROpDALo-M/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTIxMzIy/OTctMTM5MTI5ODMw/Ni03ODg4LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Various - Mythical Tapes Vol. I', 'cover_image_link': 'https://i.discogs.com/WU-cIgUI_-3-1vFxM06SgUO4oykhn_i8XgV39Ss0A8Y/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTQ4ODA1/MTUtMTM3ODc0NDI0/Mi0zNzg1LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Yes - Yessongs', 'cover_image_link': 'https://i.discogs.com/Ifdi30FMqrHGtfnE2CkzYqONmmN0YdRPgcNND2KP6Wg/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTc0MzYx/My0xNDQ4MjA2NTU3/LTQ4NjUuanBlZw.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': \"Marc Hamilton - Disque D'Or\", 'cover_image_link': 'https://i.discogs.com/GIqSPWHd803G_Q6maD6Yvtd0Imgoem5ylfWhZNnK6TY/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgxODM1/NDUtMTQ4MTExNzE0/NS00MjUwLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Marc Hamilton - Viens', 'cover_image_link': 'https://i.discogs.com/fu6BkMm-PdvYUIu4tlcWN97eBaO0gTTTPzhyf3AgIJc/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTY4NTEz/NjAtMTQyNzk4MzMy/OC00MTg3LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Татьяна Гринденко*, Юрий Смирнов* - Ragtimes = Рэг-Таймы', 'cover_image_link': 'https://i.discogs.com/jW-sU9hijnGoRzsWUTTfsVPXcIo_3UzL8bjsHNYGzVs/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTIzNDY1/OTk5LTE2NTQzNjU2/NjItOTQyNC5qcGVn.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'バニラ・ファッジ* = The Vanilla Fudge* - キープ・ミー・ハンギング・オン = You Keep Me Hanging On', 'cover_image_link': 'https://i.discogs.com/Yix2DeX7xATDHLgyIDX0kfTvbyix22IZ1mJldz5fhO4/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTc3MTk2/ODItMTQ3MzE0Nzgw/OS04NTQzLnBuZw.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'The Lovethugs - Playground Instructors', 'cover_image_link': 'https://i.discogs.com/P5pXiTahIieygcpD_H-vBzo2ft5b15MY-zlwezOxYt0/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgyMDI3/MzctMTQ1NzA1ODk1/Ni0zOTMzLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': \"Various - Blanck Mass Presents The Strange Colour Of Your Body's Tears Re-Score\", 'cover_image_link': 'https://i.discogs.com/PrlqzNQYvr3xbri7MH_pROstAHyp2vg6LSQsZRo4xAU/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTcyNTE1/OTctMTQzNzIwMDQy/NC0yMjE3LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Jing Chi, Vinnie Colaiuta, Robben Ford, Jimmy Haslip - Jing Chi', 'cover_image_link': 'https://i.discogs.com/xyYBiZR__1HkJURkzLL70fnMe1xYVvElqDSDvarIsm8/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTY2Njc2/ODItMTQyNDIxNTQ5/OS05Nzk0LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Rock Shop - Mr. Lee\\'s \"Swing\\'n Affair\" Presents', 'cover_image_link': 'https://i.discogs.com/4XVsifu2kpFu_nRCCrfsh9XtWPmtelABo2-5lBtlkrE/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgyMTQz/MDMtMTQ1NzI3OTEz/Ni0xMzQ0LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Blue Mink - Melting Pot', 'cover_image_link': 'https://i.discogs.com/VBSWqKSL0T6Bre2rMjQaovN_uEMjii0AoLJ_WYO5ejQ/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTQ3MTI2/NDUtMTU4NjIyMDEz/NC03NzMzLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Ars Nova (3) - Pavan For My Lady', 'cover_image_link': 'https://i.discogs.com/lgp0AuyykuN1KzQ4xgLHsR8J14xlJ4WFF7xhipEmD70/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgyNDQy/NDEtMTQ1Nzg0Nzgy/MC00ODY1LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Blues Pills - Blues Pills', 'cover_image_link': 'https://i.discogs.com/-aqQ84mnWf9yyEkvLIDK-TBnAmF2o0wMvX9x1RiJqUE/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTU5MTE4/MTItMTQxMDUwOTM5/My0xNjk2LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'The Beatles - Lady Madonna', 'cover_image_link': 'https://i.discogs.com/XFYQKYDBy0iocQokZuF7IM30msiuc1TZ0jUkTnimHnI/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgyNTA1/NzktMTQ1Nzk2NjM2/Ny03MDcyLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': \"Sleep - Reunion At All Tomorrow's Parties\", 'cover_image_link': 'https://i.discogs.com/z4eJHngDwGIPyW0vkhvorI23aHz15_PsDQgbj6MQQmQ/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTUwMTc1/OTQtMTM4MjI1NjI3/My01NzAzLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Reel People Feat. Darien - Sure', 'cover_image_link': 'https://i.discogs.com/7OuCn186O3qqmCDHAe69E-ZgOUVQuHph6K4Fu-E8aaA/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTczODA0/NjEtMTQ0MDI1MTcz/Ni04MTA3LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Grim Fandango - In The Rough', 'cover_image_link': 'https://i.discogs.com/Ra1prFJ89i-XB1C50kMDTsMoOhPnGr52K2WsR9HFPmU/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTU3NzE5/MzgtMTQwMjIzMDM3/Ny0yNjcyLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Paul Mauriat And His Orchestra - Let The Sunshine In / Midnight Cowboy / And Other Goodies', 'cover_image_link': 'https://i.discogs.com/N_eBaAMU8W-WeaKeiku1Kayn6du0VMtnHVjWMe_1Xak/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTE2MjUw/NjgtMTM1Nzk1MTA0/OC0yNjUzLmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'The Sea Urchins - A Morning Odyssey', 'cover_image_link': 'https://i.discogs.com/mGrVS9S4ckiztRGAagYRXN6ZePsU1o4WZ7mkvFDI0vY/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTk5NDA5/Ny0xMTg2MjE1NjM1/LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Various - Re-Evolution: FDM Sings The Hollies', 'cover_image_link': 'https://i.discogs.com/S_iTIJYinLqXOz8BFx5CSGV9P1FcxvZhz2HWascMMNw/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTQzNjIw/NjctMTM2NDc0MTM2/OC04MTM4LmpwZWc.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.discogs.com/lists/277616>\n","{'uri': 'https://www.discogs.com/lists/277616', 'album_title': 'Josephine Foster - Hazel Eyes, I Will Lead You', 'cover_image_link': 'https://i.discogs.com/2cQNQSUfjJyD_d78We5T3XHbl_GzvuHxwhfmegDK3YM/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTEwNDg4/OTktMTI5MjMyMDUx/Ny5qcGVn.jpeg'}\n","2023-02-06 23:02:35 [scrapy.core.engine] INFO: Closing spider (finished)\n","2023-02-06 23:02:35 [scrapy.extensions.feedexport] INFO: Stored csv feed (25 items) in: discogs.csv\n","2023-02-06 23:02:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 623,\n"," 'downloader/request_count': 2,\n"," 'downloader/request_method_count/GET': 2,\n"," 'downloader/response_bytes': 20104,\n"," 'downloader/response_count': 2,\n"," 'downloader/response_status_count/200': 2,\n"," 'elapsed_time_seconds': 0.870176,\n"," 'feedexport/success_count/FileFeedStorage': 1,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2023, 2, 6, 23, 2, 35, 637304),\n"," 'httpcompression/response_bytes': 123857,\n"," 'httpcompression/response_count': 2,\n"," 'item_scraped_count': 25,\n"," 'log_count/DEBUG': 30,\n"," 'log_count/INFO': 11,\n"," 'log_count/WARNING': 1,\n"," 'memusage/max': 67518464,\n"," 'memusage/startup': 67518464,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 1,\n"," 'scheduler/dequeued/memory': 1,\n"," 'scheduler/enqueued': 1,\n"," 'scheduler/enqueued/memory': 1,\n"," 'start_time': datetime.datetime(2023, 2, 6, 23, 2, 34, 767128)}\n","2023-02-06 23:02:35 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}],"source":["!cd basics; scrapy crawl discogs -O discogs.csv"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["uri,album_title,cover_image_link\r\r\n","https://www.discogs.com/lists/277616,Maroon 5 - Songs About Jane,https://i.discogs.com/9S5gwiP7Jr5Emfdrl8ERBHxUAc4p-mPHU8GMbGbGees/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTQzNTEz/OC0xMTEzMDM2MzE1/LmpwZw.jpeg\r\r\n","https://www.discogs.com/lists/277616,Hawkwind - Space Ritual,https://i.discogs.com/wMzYu3lEPnNPB8Ord6SyhoA6eZSvgu8ZQMSxRxO34Dw/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTY0MDQw/Ni0xNjA4MTM0NTcx/LTE0NzkuanBlZw.jpeg\r\r\n","https://www.discogs.com/lists/277616,Cave Gaze Wagon - Wonderful Wagon World,https://i.discogs.com/wyMbzqKbQSMjcdkWowSrSRX7Pj_m5phhSxw5ZFe3oE4/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgxNTk3/OTctMTQ1NjI1MjI2/Ni00MDg1LmpwZWc.jpeg\r\r\n","https://www.discogs.com/lists/277616,The Pax Cecilia - Nouveau (A Theatre Of The Air),https://i.discogs.com/tfzq-AQIFomVcMDxegx4a8pVCgjn9_2CdDROpDALo-M/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTIxMzIy/OTctMTM5MTI5ODMw/Ni03ODg4LmpwZWc.jpeg\r\r\n","https://www.discogs.com/lists/277616,Various - Mythical Tapes Vol. I,https://i.discogs.com/WU-cIgUI_-3-1vFxM06SgUO4oykhn_i8XgV39Ss0A8Y/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTQ4ODA1/MTUtMTM3ODc0NDI0/Mi0zNzg1LmpwZWc.jpeg\r\r\n","https://www.discogs.com/lists/277616,Yes - Yessongs,https://i.discogs.com/Ifdi30FMqrHGtfnE2CkzYqONmmN0YdRPgcNND2KP6Wg/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTc0MzYx/My0xNDQ4MjA2NTU3/LTQ4NjUuanBlZw.jpeg\r\r\n","https://www.discogs.com/lists/277616,Marc Hamilton - Disque D'Or,https://i.discogs.com/GIqSPWHd803G_Q6maD6Yvtd0Imgoem5ylfWhZNnK6TY/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTgxODM1/NDUtMTQ4MTExNzE0/NS00MjUwLmpwZWc.jpeg\r\r\n","https://www.discogs.com/lists/277616,Marc Hamilton - Viens,https://i.discogs.com/fu6BkMm-PdvYUIu4tlcWN97eBaO0gTTTPzhyf3AgIJc/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTY4NTEz/NjAtMTQyNzk4MzMy/OC00MTg3LmpwZWc.jpeg\r\r\n","https://www.discogs.com/lists/277616,\"Татьяна Гринденко*, Юрий Смирнов* - Ragtimes = Рэг-Таймы\",https://i.discogs.com/jW-sU9hijnGoRzsWUTTfsVPXcIo_3UzL8bjsHNYGzVs/rs:fill/g:sm/q:40/h:300/w:300/czM6Ly9kaXNjb2dz/LWRhdGFiYXNlLWlt/YWdlcy9SLTIzNDY1/OTk5LTE2NTQzNjU2/NjItOTQyNC5qcGVn.jpeg\r\r\n"]}],"source":["!cd basics; head discogs.csv"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"790dd6c57fc929fd42e5ab267bfea63b216295e4d85fc21cf4d3027566e8b5a9"}}},"nbformat":4,"nbformat_minor":2}
